\subsection{Normative ethical perspective on CA}

Ethics is defined as "the systematic reflection of morality"\cite[ch. 3.2]{Ethics_textbook}; morality is the notion of right and wrong as expressed by the thoughts, actions and decisions of an individual or a collective. The introduction of CA technology in society leads to a vast array of ethical issues. In this essay the perspective of normative ethics will be applied to look at how ethical theories may consider CA to be moral or immoral; the three major ethical theories applied will be Utilitarianism, Duty ethics and Virtue ethics.

% Following this three other ethical issues that will arise due to cybernetic augmentation will be discussed. These issues are the Collinridge dilemma, the ethical principles of engineers on cybernetic augmentation development and the effect on responsibility and blameworthiness of individuals due to augmentations. 

%\subsubsection{}

\subsubsection{Utilitarianism}

"In utilitarianism, actions are judged by the amount of pleasure and pain they bring about. The action that brings the greatest happiness for the greatest number should be chosen." \cite[ch. 3.7]{Ethics_textbook} Further addition that need to be made to refine this theory is the Freedom principle, which states that people are free to pursue their own pleasures as long as it does not hinder others. 

For CA to be acceptable according to utilitarianism the net benefits need to outweigh the net risks. This is something that will be hard, if not impossible, to establish with scientific studies. The effect of the Collingridge dilemma {\bf CITE TO COLLINGRIDGE,1980 AS IN PG 32 OF BOOK} is extensively pronounced in the case of CA research and implementation due to both the potential unknown capability and the extensively strong effect it may have on humanity's development. Notions of Constructive Technology Assessment (CTA) {\bf CITE TO SCHOT AND RIP 1997 AS IN PG 32 OF BOOK} can be possibilities to guide its progress, but as mentioned before, due to the unique nature of CA the technology can easily spiral out of assessed development cycles. 

%There are also issues with the freedom principle and the notion of distributive justice regarding the applications of CA. The freedom principle would require that one inform and get consent from both the users of CA and the society of its potential risks; this problem is further exacerbated by the notion that currently, due to globalization the society implies the sum of humanity. Not only the risks hard to discover without violating the freedom principle, once they are discovered, it will be very hard to inform the people of the complex nature of the risks associated with CA. Moreover, the fact that CA can give individuals greater capabilities over non-enhanced can violate the freedom principle and distributive justice. If someone has an advantage that makes her better than the rest at getting a job due to CA, than the ability of non-enhanced people to get that job is being hindered. CA is not only increasing the freedoms of the enhanced, due to this tipping, the already problematic distribution of goods and services will get worsened, violating principles of distributive justice.

However, in the end it must be realized that greater utility is what has driven technological progress; CA will lead to a net increase in human potential. If too many social costs are not incurred, an equitable distribution of CA can be achieved maintaining the rights and liberties of the individuals, it can easily be imagined how CA would lead to a better life for its bulk.

%maybe make this part shorter

\subsubsection{Duty ethics}

Duty ethics judges actions to be right or wrong based on how much they agree to a moral norm. These norms are described by duty ethics as "Categorical imperative"; there are two primary categorical imperatives in duty ethics; namely the "Universality principle" and the "Reciprocity principle" \cite[ch. 3.8]{Ethics_textbook}. The universality principle states "Act only on that maxim which you can at the same time will that it should become a universal law". The reciprocity principle states "Act as to treat humanity, whether in your own person or in that of any other, in every case as an end, never as means only" \cite[ch. 3.8]{Ethics_textbook}.

A big issue with the application of duty ethics, more specifically the universality principle, will be the "slippery slope argument" {\bf CITE TO BURG 1991 AND RESNIK 1994 AS IN PG 26 OF ANDY MIAH}; this argument states that if permitting a good action X creates precedent for permitting a bad action Y, X should not be permitted. Due to the nature of CA it is unlikely that a rigid framework of universal laws will be applicable. More case by case deliberation will be required to judge if they are right or wrong. More problems can be seen if we apply the second categorical imperative to the analysis; it can be applied to CA in two opposite ways. Firstly, it can be argued that CA causes human beings to lose humanity (i.e. become more machine like) and thus may encourage the tendency of treating them like means to achieve certain ends. On the other hand, if one adopts the view that CA will be a means that humanity will use to aid or improve itself with negligible risk to itself, than this imperative is no longer violated.



\subsubsection{Virtue ethics}






If 

%Instead, one should be aware of the fact that such ethical issues are often very abstract and subjective, as it directly deals with the moral values of the people which is reflected on the zeitgeist of the society. 

%One may dispute against this perspective by using an analogy to other currently available technology (e.g. "The cars enabled us to move faster, and the computers allowed for better management of information. No one says cars and computers damage our human dignity. The same should go for cybernetic augmentation." However, such argument is again subject to disputes as the cybernetic augmentation takes an intrusive form and becomes "as if it was a natural part of the human body". Therefore, the analogy between the cybernetic augmentation and currently existing technologies is not perfectly justifiable.

% % % NOTES

%## technology assessment, collinridge dilemma,CTA; pg 32 ch1
% maybe some kind of talk about responsibility ch1 & 8
%# Ethhical principles for engineers in a global environment (Luegenbiehl,2010); pg 55 ch2
%## quote no harm principle and freedom principle when used subsubsection of acceptability of risk; pg74-5 ch3 ethics book
%## use concepts of distributive justice and marginal utility; pg 76 ch3, ethics book
% use the concept of eudamonia from aristotles vitue ethics; pg 84 ch3, ethics book
%## issue of the slipperly slope argument as in some paper i read; issue of similarity to the following quote if we choose for CA 'Vaughan’s analysis of the Challenger disaster illustrates a more general point: decisions – also incremental and implicit ones – tend to commit us to certain courses of actions and frame subsequent decisions (Darley, 1996).' pg 145 ethics book
% ethics during approach to design i.e. responsibility of researchers and designers; problem of 'radical design concept in CA, this maybe part of risks aswell. ch 6;
% technological mediation, moralizing of technology ch 7. moralizing of tech mentioned in benefits part
% strong presence of both uncertainity and ignorance in CA, the ignorance can be more conseuquential. this point maybe more appropriate in risks part ch 8




